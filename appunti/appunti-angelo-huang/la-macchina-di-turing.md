---
ShowToc: true
TocOpen: false
tags:
- theoretical-computer-science
title: La macchina di Turing
weight: 21
---

### Introduzione
#### Note filosofiche (non impo)
Bisogna in primo momento cercare di definire **cosa √® la computazione** e cosa √® un computer.
Aristotele faceva la distinzione fra propriet√† **essenziali** e **accidentali**. Quelle essenziali sono proprie dell'oggetto.
> Una sedia pu√≤ essere fatta di legno o di metallo, ma questa propriet√† √® accidentale, ovvero, essa rimane una sedia indipendentemente dal materiale di cui √® fatta.

Solitamente in matematica si prova ad astrarre (vedi Astrazione sul controllo per nota generale sull'astrazione). Per√≤ in questo campo si sono trovati molte concezioni equivalenti. Fino ad arrivare a concepire la tesi di Church-Turing. Il prof. nota che questo √® strano, perch√© in altre discipline si converge in unico modello, mentre qui molte cose sono indifferenti.
Questo √® importante per capire come la concezione di Computer Science si √® evoluta [(Denning 2010)](https://dl.acm.org/doi/10.1145/1880066.1880067).

#### Nascita della calcolabilit√†: Turing (curiosit√†)
Al tempo si voleva in matematica trovare un formalismo, un fondamenta logico alla matematica che poteva permettere di dimostrare tutto dalle fondamenta. Il contributo principale, e secondo il prof il lavoro pi√π importante in tutta l'informatica era [(Turing 1937)](http://doi.wiley.com/10.1112/plms/s2-42.1.230), che definisce cosa significa calcolare un problema. Questo porta al significato di calcolare un problema.
√à interessante notare come Turing arriva al suo formalismo.
Osserva
1. L'esecuzione di certe regole
2. Un foglio di carta in cui sono lette e scritte dei simboli
3. L'azione eseguita dipende dal simbolo precedente

Da queste osservazioni iniziali, hanno *astratto i dati* (ora simboli binari) e *le azioni*, un set molti semplice.
Osservazione: **computazione √® locale**.
Questo sembra simile quanto dichiarato in [(Dehaene 2014)](https://books.google.it/books/about/Consciousness_and_the_Brain.html?id=9c02BwAAQBAJ&redir_esc=y) quando si parla che l'essere umano √® capace di porre davanti l'attenzione della coscienza solamente un solo simbolo alla volta.

### La macchina di Turing
Vedere precedente per capire come √® stata ideata questa macchina di Turing.

#### Definizione matematica üü©
√à interessante confrontare questa definizione con [Fondamenti teorica#La macchina di turing](./fondamenti-teorica#la-macchina-di-turing) in cui usiamo un formato leggermente diverso, il formato preciso √® quello precedente fatto a linguaggi. In sto corso usiamo un formalismo pi√π semplice).
Nel nostro caso √® una 5-tupla di
- $\Sigma$, un **alfabeto** di simboli finiti, con simbolo speciale per cella vuota
- $Q$ Un insieme di stati
- $q_{0} \in Q$ lo stato iniziale
- $H \subseteq Q$  l'insieme degli stati finali
- $\delta$ la funzione di transizione che soddisfa questo:
$$
\delta : (Q - H) \times \Sigma \to Q\times \Sigma \times \left\{ \to, \leftarrow \right\} 
$$
La differenza √® che in questo caso l'alfabeto dell'input √® uguale all'alfabeto del nastro, ma alla fine cambia poco, basta TODO (capire) secondo me ha sbagliato il prof. tempo fa, perch√© l'alfabeto di input non √® mai preso in considerazione nella funzione di transizione boh.

Questa sintassi √® pi√π comprensibile della precedente quindi nice.

Come per tutti i precedenti  automi, anche questi hanno una rappresentazione possibile a diagramma:
![La macchina di Turing-20240221114813908](./static/images/la-macchina-di-turing-20240221114813908.webp)

A lezione abbiamo anche visto esempi di macchine che computano moltiplicazione binaria o addizione binaria.

### Problemi di decisione
#### Definizione problemi di decisione üü©
Molti problemi si possono codificare attraverso un problema di decisione, ossia un problema in cui abbiamo solamente bisogno di una risposta s√¨ o no.
Se riusciamo a codificarlo con il linguaggio delle macchine di Turing, allora forse si pu√≤ far verificare alla macchina. Si vedr√† che molti problemi non vanno.

#### Schema di codifica
Ossia un dato $\alpha$ sar√† descritto con $code(\alpha) \in \Sigma^{*}$. Una nota interessante √® che con Kolmogorov complexity abbiamo un dato di lunghezza minima, qui vediamo bene il link molto diretto.
Questo ha delle propriet√†:
1. √à **Iniettiva**
2. Vorremmo capire in $\Sigma^{*}$ quali siano dei codici possibili per un qualche $\alpha$ nel nostro dominio.
3. Sarebbe carino poter ritrovare $\alpha$ a partire dal suo codice.

#### Definizione decidibilit√† üü©
Dato un certo linguaggio, supponiamo di avere una macchina di Turing come definita di sopra [#La macchina di Turing](#la-macchina-di-turing) tale per cui abbia due stati finali $\left\{ H, N \right\}$, allora diciamo che $\mathcal{M}$ **decide** $L$ se vale che
- Quando $x \in L$ allora $\mathcal{M}$ accetta $x$, ossia finisce su stato $H$ 
- Quando $x \not\in L$ allora $\mathcal{M}$ rigetta $x$, ossia finisce su stato $N$

Diciamo che un linguaggio $L$ √® decidibile se una macchina di Turing lo decide.
Ora abbiamo formalizzato il significato di decidibilit√†.
Un altro modo per dire che una macchina √® decidibile √® se **si ferma per ogni input**, questo significa che o finisce in stato accettante o in stato rigettante.

#### Definizione riconoscibilit√†/semidecibilit√† üü©
Uguale al precedente, con la differenza che quando la stringa non appartiene al linguaggio **diverge**.

Diciamo un linguaggio $L$ √® riconoscibile se una macchina di Turing lo riconosce.
Th: **Decidibilit√† -> Riconoscibilit√†**, √® facile costruire una tale macchina di Turing partendo da una che la decide, possiamo estendere il caso negativo in questo modo: se raggiungo lo stato negativo allora vado in loop infinito a caso.

#### Definizione non riconoscibilit√†
Significa che non pu√≤ dire in tempo finito n√© s√¨ n√© no. Quindi √® una cosa ancora pi√π forte rispetto la semidecibilit√†.

#### Gerarchia di Chomskyüü®+
Vedere [Linguaggi liberi e PDA#Classificazione dei linguaggi](./linguaggi-liberi-e-pda#classificazione-dei-linguaggi) alla sezione schema generale delle grammatiche. La cosa da ricordare √® che TM √® il modello pi√π generale fra tutti i precedenti modelli di macchine di Turing e automi.
![La macchina di Turing-20240512120403415](./static/images/la-macchina-di-turing-20240512120403415.webp)
### Tesi di Church-Turing
#### Enunciato della tesiüü©
> Se la soluzione di un dato problema pu√≤ essere calcolata attraverso una procedura algoritmica, allora pu√≤ essere calcolata da una macchina di Turing. (*Alonzo Church*)

Alonzo proponeva una altra teoria di calcolo, √® stato proponente di *lambda calcolo*. Quelli che piacevano a Asperti.

Storicamente sembra vero, perch√© sempre prendendo qualcosa che sembra pi√π espressibile, resta alla fine equivalente a turing.

Esempi di conseguenze:
- Macchine di Turing 'migliorate' (nondeterministiche, probabilistiche, pi√π nastri‚Ä¶) 
- Macchine a registri
- Linguaggi di programmazione di alto livello come Python, Java, C, ‚Ä¶ 
- (Codice macchina di) computer classici ‚Ä¢ Computer quantistici

#### Espressibilit√† vs semplicit√† e efficienzaüü©

Probabilmente lo abbiamo citato in Fondamenti teorica, il fatto che questo √® solamente una *congettura* perch√©  non √® possibile codificare tutti i formalismi possibili.
Parla di **espressibilit√†** ossia cosa pu√≤ essere calcolato, e se questo vale permette di dire che √® una macchina universale.
Infatti **non dice nulla su semplicit√† o efficienza** dell'algoritmo (in questa astrazione non ci interesssa).

#### Giustificazione valenza di Turing (non impo)
La cosa interessante di queste macchine comunque √® che [La macchina di Turing](./la-macchina-di-turing) ci permette di formalizzare!
1. Rigorosit√† di algoritmo. (anche se non mi sembra buono per esprimere certe forme di calcolo [(Denning 2010)](https://dl.acm.org/doi/10.1145/1880066.1880067)).
2. Teoremi di calcolabilit√† possono essere estese a qualunque altro formalismo, se vale.

#### Versione rafforzata
Questa versione √® una estensione della tesi di Church-Turing in modo che comprenda la parte in [Time and Space Complexity](./time-and-space-complexity).

>Ogni modello di calcolo deterministico
fisicamente realizzabile pu√≤ essere simulato da
una TM (deterministica, su nastro singolo) con
overhead al **pi√π polinomiale**.

Se ci pensiamo questa versione rafforzata √® simile a quanto dimostrato in Kolmogorov complexity riguardo la complessit√† sulla lunghezza della minima stringa che lo descrive, perch√© l√¨ abbiamo un overhead al massimo costante per tradurre da una macchina all'altra.

### Chiusura del linguaggio di TM

#### Chiusura sulla decidibilit√†üü©
**Complemento:** √® molto semplice decidere sul complemento perch√© basta scambiare gli stati finali
**Unione:** basta eseguirlo sul multinastro e comparare l'input, vedi [Estensioni di Turing e altre macchine](./estensioni-di-turing-e-altre-macchine).
**Intersezione**: Usi de morgan con i precedenti.
**Concatenazione**: stessa dimostrazione per Grammatiche Regolari, concateni le macchine.
**Star**: s√¨

Un esercizio √® dettagliare la definizione di queste macchina, in modo simile a quanto facevamo al corso di linguaggi.

NOTA: ricorda di seguire la struttura della dimostrazione. Se no te la conter√† come sbagliata all'esame.

Per la concatenazione √® un po' pi√π difficile del previsto, perch√© **non so bene dove devo andare a tagliare** per la seconda stringa, quindi vado in modo non deterministico sul taglio, cos√¨ in pratica faccio tutte le cose possibili.
#### Chiusura sulla riconoscibilit√† üü©
**Complemento**: No
**Unione** s√¨, stessa cosa precedente.
**Intersezione**: credo basti concatenarli con reset dell'input e dire che si accetta se arriva in fondo
**Concatenazione**: s√¨
**Star**: dovrebbe s√¨.

La non chiusura del complemento la trovi in [Halting Theorem and Reducibility](./halting-theorem-and-reducibility). Perch√© si dimostra che $HALT$ e il suo complementare non sono chiusi, e questo basta per il complemento.
### La macchina di Turing universale

#### Esempi di macchine universali
Sono dei programmi in grado di eseguire altri programmi. √à una cosa molto particolare dell'informatica questa cosa, permetterebbe per esempio di eseguire un programma su se stesso, una cosa ricorsiva (oroboro quasi).
Esempi di macchine universali possono essere
1. Sistemi operativi
2. Stack di hardware che abbiamo, ognuna universale che esegue una sopra l'altra.

Anche questo √® stato pensato in [(Turing 1937)](http://doi.wiley.com/10.1112/plms/s2-42.1.230).
Una cosa interessante √® che prima di esso, la macchina era pensata per una unica cosa, dopo Turing si pu√≤ usare la stessa macchina per tutti gli algoritmi possibili. Ha introdotto la nozione di programmabilit√†!
Utilizzare il dato (l'algoritmo) come input di s√© stesso √® stato usato da G√∂del nella sua dimostrazione famosa. Ha codificato teoremi come numeri, permettendo l'uso dell'aritmetica stessa.
#### Descrizione UTMüü©
![La macchina di Turing-20240229124153090](./static/images/la-macchina-di-turing-20240229124153090.webp)
La cosa importante √® che:
> La macchina universale deve avere lo stesso comportamento di $\mathcal{M}$.

Se si ferma, si ferma con stesso output, altrimenti non si ferma.

#### Costruzione di UTMüü©
Codifichiamo simboli che possono apparire nella definizione di $\delta$, per esempio
$\sigma_{0} = \cup, \sigma_{1} = \leftarrow, \sigma_{2} = \to$ e poi in modo simile per ogni simbolo dell'alfabeto finito.
Poi possiamo codificare in modo unario, per esempio $code(q_{i}) = 111..11$ per  $i+1$ volte in totale, in modo simile per $\sigma_{i}$. Poi uso gli 0 per separare i codici.
Quindi posso avere una singola transizione, che √® una tupla di simbolo in input, stato input, stato output, simbolo output, e movimento.
Quindi
$$
code(t) = code(q_{i})0code(\sigma_{n})0code(q_{j})0code(\sigma(m))0code(\sigma_{o})0
$$
Poi possiamo separarli con uno 0 in pi√π, o contare quanti 0 hai visto.
Si pu√≤ encodare anche l'input, in modo simile
$$
code(\sigma_{1i}...\sigma_{in}) = 00code(\sigma_1)0 code(\sigma_{12}) 0 ... 0 code(\sigma_{in}).
$$
√à interessante osservare che questo formato $code(\mathcal{M})code(i)$ √® una cosa decidibile, perch√© √® un formato che si conosce.

1. Lettura simboli macchina specifica
2. Interpretazione di essa
3. Poi si pu√≤ continuare ad utilizzare il nastro per simulare la macchina stessa.

Per cominciare una simulazione della UTM:
1. Passo di preparazione: verifica che y = code(‚Ñ≥)code(x) per
qualche TM ‚Ñ≥ e input x. Se no, cicla. Se si, allora nastro 1 contiene code(‚Ñ≥)code(x). Vai al passo 2.
2. Sposta code(‚Ñ≥) dal nastro 1 al nastro 2. Ora nastro 1 mostra il contenuto del nastro di ‚Ñ≥ su input x alla configurazione iniziale, in forma codificata. 
3. Scrivi code(q0) su nastro 3. 
4. Posiziona testina 1 sul primo simbolo di code(x), testina 2 sul primo simbolo di code(‚Ñ≥), and testina tre sul primo simbolo di code(q0).

Quindi poi:
Cerco la funzione transizione corretta sul nastro 2 che contiene la codifica, poi aggiorno i nastri 1 e 3 a seconda di cosa scrivo, della testina e dello stato corrente in nastro 3.



# References

[1] Denning [‚ÄúUbiquity Symposium 'What Is Computation?': Opening Statement‚Äù](https://dl.acm.org/doi/10.1145/1880066.1880067) Ubiquity Vol. 2010, pp. 1880066.1880067 2010

[2] Turing [‚ÄúOn Computable Numbers, with an Application to the Entscheidungsproblem‚Äù](http://doi.wiley.com/10.1112/plms/s2-42.1.230) Proceedings of the London Mathematical Society Vol. s2-42(1), pp. 230--265 1937

[3] Dehaene [‚ÄúConsciousness and the Brain‚Äù](https://books.google.it/books/about/Consciousness_and_the_Brain.html?id=9c02BwAAQBAJ&redir_esc=y) Singapore Books 2014
